{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸŒ¼ RAGê¸°ë²•ì˜ ì´í•´ì™€ ì ìš© - 3ì°¨ì‹œ(24.12.02)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RMARKET\\AppData\\Local\\Temp\\ipykernel_2404\\3559429672.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "# ë©”ëª¨ë¦¬ì— inputìœ¼ë¡œ ìš°ë¦¬ì˜ ì‘ë‹µ, outputìœ¼ë¡œ AIì˜ ì‘ë‹µì„ ë‹´ì„ ìˆ˜ ìˆë‹¤\n",
    "# FIFO\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    inputs = {\n",
    "        'human': 'ì•ˆë…•í•˜ì„¸ìš”, íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ëŸ¬ ì™”ìŠµë‹ˆë‹¤.'\n",
    "    },\n",
    "    outputs = {\n",
    "        'ai': 'ì•ˆë…•í•˜ì„¸ìš”, ìƒˆ íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ì‹ ë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: ì•ˆë…•í•˜ì„¸ìš”, íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ëŸ¬ ì™”ìŠµë‹ˆë‹¤.\\nAI: ì•ˆë…•í•˜ì„¸ìš”, ìƒˆ íœ´ëŒ€í°ì„ êµ¬ë§¤í•˜ì‹ ë‹¤ë‹ˆ ê¸°ì©ë‹ˆë‹¤.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "langchain_practice\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"langchain_practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', 'ë„ˆëŠ” ì¹œì ˆí•œ ì±—ë´‡ì´ì•¼'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),  # AI ì‘ë‹µ ìë¦¬ ë§Œë“¤ì–´ì£¼ê¸°\n",
    "        ('human', '{input}')  # ë™ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì²˜ë¦¬\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history = RunnableLambda(memory.load_memory_variables) | itemgetter('chat_history')\n",
    "    # ë©”ëª¨ë¦¬ì˜ ëŒ€í™” ë‚´ìš©ì„ ë¡œë“œí•˜ê³ , ë©”ëª¨ë¦¬ì˜ ëŒ€í™” ê¸°ë¡ì¸ chat_history(key)ë§Œ get\n",
    "    # ì „ì²´ ì…ë ¥ ë°ì´í„°ì—ì„œ chat_historyë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ì•ˆë…•í•˜ì„¸ìš”, ìˆ˜ë¹ˆë‹˜! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 36, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'stop', 'logprobs': None} id='run-2b7f0c46-0ded-4c1e-a127-28d4de3e9eb1-0' usage_metadata={'input_tokens': 36, 'output_tokens': 21, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'input': 'ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ ìˆ˜ë¹ˆì…ë‹ˆë‹¤.'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'human': 'ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ ìˆ˜ë¹ˆì…ë‹ˆë‹¤.'}, {'ai': response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({'input': 'ì œ ì´ë¦„ì´ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ ê¸°ì–µí•˜ì‹œë‚˜ìš”?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({'human': 'ì œ ì´ë¦„ì´ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ ê¸°ì–µí•˜ì‹œë‚˜ìš”?'}, {'ai': response.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ ìˆ˜ë¹ˆì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ìˆ˜ë¹ˆë‹˜! ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='ì œ ì´ë¦„ì´ ë¬´ì—‡ì´ì—ˆëŠ”ì§€ ê¸°ì–µí•˜ì‹œë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ë„¤, ìˆ˜ë¹ˆë‹˜ì´ë¼ê³  í•˜ì…¨ì£ . ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RMARKET\\AppData\\Local\\Temp\\ipykernel_2404\\1883192381.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1, return_messages=True, memory_key='chat_history')\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1, return_messages=True, memory_key='chat_history')\n",
    "# k: ëª‡ ê°œì˜ ê¸°ë¡ì„ ê¸°ì–µí•  ê²ƒì¸ì§€\n",
    "# return_messages: ë©”ëª¨ë¦¬ì—ì„œ ë°˜í™˜ë˜ëŠ” í˜•ì‹ì„ ê°œë³„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ (False: í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_chain(memory, query):\n",
    "    chat_history = RunnablePassthrough.assign(\n",
    "        chat_history = RunnableLambda(memory.load_memory_variables) | itemgetter(memory.memory_key)\n",
    "    )\n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            ('system', 'ë„ˆëŠ” ì¹œì ˆí•œ AI ë¹„ì„œì•¼'),\n",
    "            MessagesPlaceholder(variable_name='chat_history'),\n",
    "            ('human', '{input}')\n",
    "        ]\n",
    "    )\n",
    "    chain = chat_history | prompt | llm | StrOutputParser()\n",
    "    # StrOutputParser: ì¶œë ¥ë˜ëŠ” ê°’ì„ ìë™ìœ¼ë¡œ íŒŒì‹± (ë¶ˆí•„ìš”í•œ ê°œí–‰ë¬¸ì, íŠ¹ìˆ˜ê¸°í˜¸ ë“±ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "\n",
    "    answer = chain.invoke({'input': query})\n",
    "    memory.save_context(inputs={'human': query}, outputs= {'ai': answer})\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai :  ì¶”ìš´ ë‚ ì”¨ì—ëŠ” ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•´ì¤„ ìˆ˜ ìˆëŠ” ìŒì‹ì„ ë¨¹ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìŒì‹ì„ ì¶”ì²œí•´ë“œë¦´ê²Œìš”:\n",
      "\n",
      "1. **ê¹€ì¹˜ì°Œê°œ**: ë§¤ì½¤í•˜ê³  ëœ¨ê±°ìš´ êµ­ë¬¼ì´ ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•´ì¤ë‹ˆë‹¤. ë¼ì§€ê³ ê¸°ë‚˜ ë‘ë¶€ë¥¼ ë„£ì–´ ë”ìš± í’ì„±í•˜ê²Œ ì¦ê¸¸ ìˆ˜ ìˆì–´ìš”.\n",
      "\n",
      "2. **ëœì¥ì°Œê°œ**: êµ¬ìˆ˜í•œ ë§›ì˜ ëœì¥ì°Œê°œëŠ” ë‹¤ì–‘í•œ ì±„ì†Œì™€ ë‘ë¶€ë¥¼ ë„£ì–´ ì˜ì–‘ê°€ë„ ë†’ê³ , ì†ì„ ë“ ë“ í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "3. **ì‚¼ê³„íƒ•**: ë‹­ê³ ê¸°ì™€ ì¸ì‚¼, ëŒ€ì¶” ë“±ì„ ë„£ì–´ ë“ì¸ ì‚¼ê³„íƒ•ì€ ëª¸ì„ ë”°ëœ»í•˜ê²Œ í•˜ê³  ê¸°ìš´ì„ ë¶ë‹ì•„ ì¤ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì¹¼êµ­ìˆ˜**: ëœ¨ê±°ìš´ êµ­ë¬¼ì— ì«„ê¹ƒí•œ ë©´ë°œì´ ì–´ìš°ëŸ¬ì§„ ì¹¼êµ­ìˆ˜ëŠ” ì¶”ìš´ ë‚ ì”¨ì— ì œê²©ì…ë‹ˆë‹¤. í•´ë¬¼ì´ë‚˜ ë‹­ê³ ê¸°ë¥¼ ë„£ì–´ ë‹¤ì–‘í•œ ë§›ì„ ì¦ê¸¸ ìˆ˜ ìˆì–´ìš”.\n",
      "\n",
      "5. **í˜¸ë–¡**: ë‹¬ì½¤í•œ ì‹œëŸ½ì´ ë“¤ì–´ê°„ í˜¸ë–¡ì€ ê°„ì‹ìœ¼ë¡œ ì¢‹ê³ , ë”°ëœ»í•˜ê²Œ ë¨¹ìœ¼ë©´ ê¸°ë¶„ë„ ì¢‹ì•„ì§‘ë‹ˆë‹¤.\n",
      "\n",
      "6. **ë¶•ì–´ë¹µ**: ê²¨ìš¸ì²  ê¸¸ê±°ë¦¬ ê°„ì‹ìœ¼ë¡œ ì¸ê¸° ìˆëŠ” ë¶•ì–´ë¹µì€ ë”°ëœ»í•˜ê³  ë‹¬ì½¤í•œ íŒ¥ì†Œê°€ ë§¤ë ¥ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ì™¸ì—ë„ ë”°ëœ»í•œ ì°¨ë‚˜ ì½”ì½”ì•„ë¥¼ í•¨ê»˜ ë§ˆì‹œë©´ ë”ìš± í¬ê·¼í•œ ëŠë‚Œì„ ë°›ì„ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ê°‘ìê¸° ë‚ ì”¨ê°€ ë„ˆë¬´ ì¶”ì›Œì¡Œì–´. ì´ ë•Œ ë¨¹ì„ë§Œí•œ ìŒì‹ì„ ì¶”ì²œí•´ì¤˜'\n",
    ")\n",
    "print('ai : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai :  ì¶”ìš´ ë‚ ì”¨ì— ë“£ê¸° ì¢‹ì€ ë…¸ë˜ëŠ” ë”°ëœ»í•œ ë¶„ìœ„ê¸°ë¥¼ ëŠë‚„ ìˆ˜ ìˆëŠ” ê³¡ë“¤ì´ ì¢‹ìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë…¸ë˜ë“¤ì„ ì¶”ì²œí•´ë“œë¦´ê²Œìš”:\n",
      "\n",
      "1. **ì•„ì´ìœ  - \"ë°¤í¸ì§€\"**: ì”ì”í•˜ê³  ê°ì„±ì ì¸ ë©œë¡œë””ê°€ ë§ˆìŒì„ ë”°ëœ»í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.\n",
      "\n",
      "2. **í´í‚´ - \"ëª¨ë“  ë‚ , ëª¨ë“  ìˆœê°„\"**: ë¶€ë“œëŸ¬ìš´ ëª©ì†Œë¦¬ì™€ ê°ë¯¸ë¡œìš´ ë©œë¡œë””ê°€ ì˜ ì–´ìš°ëŸ¬ì§„ ê³¡ì…ë‹ˆë‹¤.\n",
      "\n",
      "3. **ê¹€ê´‘ì„ - \"ì–´ëŠ 60ëŒ€ ë…¸ë¶€ë¶€ ì´ì•¼ê¸°\"**: ê°ì„±ì ì¸ ê°€ì‚¬ì™€ ë©œë¡œë””ê°€ ì¶”ìš´ ë‚ ì”¨ì— ì˜ ì–´ìš¸ë¦½ë‹ˆë‹¤.\n",
      "\n",
      "4. **ì—í”½í•˜ì´ - \"ì¶¥ë‹¤ (feat. ì´í•˜ì´)\"**: ì œëª©ì²˜ëŸ¼ ì¶”ìš´ ë‚ ì”¨ì— ë“£ê¸° ì¢‹ì€ ê³¡ìœ¼ë¡œ, ê°ì„±ì ì¸ ë©ê³¼ ë³´ì»¬ì´ ì¸ìƒì ì…ë‹ˆë‹¤.\n",
      "\n",
      "5. **ì¡´ ë ˆì „ë“œ - \"All of Me\"**: ë”°ëœ»í•œ ì‚¬ë‘ì˜ ê°ì •ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ë°œë¼ë“œ ê³¡ì…ë‹ˆë‹¤.\n",
      "\n",
      "6. **Sarah McLachlan - \"Winter Song\"**: ê²¨ìš¸ì˜ ë¶„ìœ„ê¸°ë¥¼ ì˜ ë‹´ì•„ë‚¸ ê³¡ìœ¼ë¡œ, ì°¨ë¶„í•˜ê²Œ ë“£ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ ë…¸ë˜ë“¤ì´ ì¶”ìš´ ë‚ ì”¨ì— ì¡°ê¸ˆì´ë‚˜ë§ˆ ë”°ëœ»í•¨ì„ ì „í•´ì¤„ ìˆ˜ ìˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ë”°ëœ»í•œ ìŒë£Œì™€ í•¨ê»˜ ê°ìƒí•´ë³´ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ê·¸ëŸ¼ ì´ëŸ´ ë•Œ ë“¤ì„ë§Œí•œ ë…¸ë˜ë¥¼ ì¶”ì²œí•´ì¤˜'\n",
    ")\n",
    "print('ai : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai :  ì£„ì†¡í•˜ì§€ë§Œ ì´ì „ì— ìŒì‹ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆˆ ì ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì–´ë–¤ ìŒì‹ì„ ë§ì”€í•˜ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ëŒ€í•œ ë ˆì‹œí”¼ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŠ¹ì •í•œ ìš”ë¦¬ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
     ]
    }
   ],
   "source": [
    "response = conversation_chain(\n",
    "    memory = memory,\n",
    "    query = 'ì•„ê¹Œ ì´ì•¼ê¸°í–ˆë˜ ìŒì‹ë“¤ ì¤‘ì—ì„œ ì²« ë²ˆì§¸ ìŒì‹ì˜ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ì¤˜'\n",
    ")\n",
    "print('ai : ', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì•„ê¹Œ ì´ì•¼ê¸°í–ˆë˜ ìŒì‹ë“¤ ì¤‘ì—ì„œ ì²« ë²ˆì§¸ ìŒì‹ì˜ ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ì¤˜', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì£„ì†¡í•˜ì§€ë§Œ ì´ì „ì— ìŒì‹ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ ë‚˜ëˆˆ ì ì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì–´ë–¤ ìŒì‹ì„ ë§ì”€í•˜ì‹œëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´ ê·¸ì— ëŒ€í•œ ë ˆì‹œí”¼ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŠ¹ì •í•œ ìš”ë¦¬ë¥¼ ì›í•˜ì‹ ë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RMARKET\\AppData\\Local\\Temp\\ipykernel_2404\\418450421.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit= 200,\n",
    "    return_messages= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì´ ì‹ë‹¹ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ì´ ì‹ë‹¹ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë©”ë‰´ëŠ” ë¶ˆê³ ê¸° ì •ì‹, í•´ë¬¼íŒŒì „, ë¹„ë¹”ë°¥, ê·¸ë¦¬ê³  ê°ìíƒ•ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶ˆê³ ê¸°ëŠ” ë‹¬ì½¤í•˜ê³  ì§­ì§¤í•œ ë§›ìœ¼ë¡œ ì™¸êµ­ì¸ ì†ë‹˜ë“¤ì—ê²Œë„ í° ì¸ê¸°ë¥¼ ëŒê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ë©”ë‰´ê°€ ì œê³µë˜ë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ë„¤, ì±„ì‹ì£¼ì˜ìë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ì±„ì†Œ ë¹„ë¹”ë°¥, ë‘ë¶€êµ¬ì´, ì•¼ì±„ì „, ê·¸ë¦¬ê³  ë‚˜ë¬¼ ë°˜ì°¬ ì„¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹ ì„ í•œ ì œì²  ì±„ì†Œë¡œ ë§Œë“¤ì–´ì ¸ ê±´ê°•í•˜ê³  ë§›ìˆëŠ” ì‹ì‚¬ë¥¼ ì¦ê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë„ ìˆë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ë„¤, ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ë¯¸ë‹ˆ ê¹€ë°¥, ë–¡ë³¶ì´, ê·¸ë¦¬ê³  ë‹¬ì½¤í•œ ê°„ì¥ ì¹˜í‚¨ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´ë“¤ì´ ì¢‹ì•„í•  ë§Œí•œ ë§›ê³¼ ê±´ê°•ì„ ê³ ë ¤í•œ ìš”ë¦¬ë“¤ì…ë‹ˆë‹¤.\"\n",
    "    },\n",
    ")\n",
    "memory.save_context(\n",
    "    inputs={\"human\": \"ì´ ì‹ë‹¹ì€ ì–´ë–¤ ë¶„ìœ„ê¸°ë¥¼ ê°€ì§€ê³  ìˆë‚˜ìš”?\"},\n",
    "    outputs={\n",
    "        \"ai\": \"ì´ ì‹ë‹¹ì€ í•œì˜¥ ìŠ¤íƒ€ì¼ì˜ ì¸í…Œë¦¬ì–´ë¡œ ì „í†µì ì¸ í•œêµ­ì˜ ë¶„ìœ„ê¸°ë¥¼ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ëœ»í•˜ê³  í¸ì•ˆí•œ ì¡°ëª…ê³¼ í˜„ëŒ€ì ì¸ ë””ìì¸ì´ ì¡°í™”ë¥¼ ì´ë£¨ì–´ ê°€ì¡± ë‹¨ìœ„ ì†ë‹˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—°ì¸ë“¤ì˜ ë°ì´íŠ¸ ì¥ì†Œë¡œë„ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='The human asks what the most popular menu items are at the restaurant. The AI responds that the most popular items are bulgogi, seafood pancake, bibimbap, and potato soup, noting that bulgogi is particularly favored by foreign guests for its sweet and salty flavor. The human then inquires about vegetarian options, and the AI confirms that they offer vegetable bibimbap, grilled tofu, vegetable pancakes, and a set of seasoned vegetables made with fresh seasonal produce for a healthy and delicious meal.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë„ ìˆë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ë„¤, ì–´ë¦°ì´ë¥¼ ìœ„í•œ ë©”ë‰´ë¡œ ë¯¸ë‹ˆ ê¹€ë°¥, ë–¡ë³¶ì´, ê·¸ë¦¬ê³  ë‹¬ì½¤í•œ ê°„ì¥ ì¹˜í‚¨ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ì´ë“¤ì´ ì¢‹ì•„í•  ë§Œí•œ ë§›ê³¼ ê±´ê°•ì„ ê³ ë ¤í•œ ìš”ë¦¬ë“¤ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì´ ì‹ë‹¹ì€ ì–´ë–¤ ë¶„ìœ„ê¸°ë¥¼ ê°€ì§€ê³  ìˆë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì´ ì‹ë‹¹ì€ í•œì˜¥ ìŠ¤íƒ€ì¼ì˜ ì¸í…Œë¦¬ì–´ë¡œ ì „í†µì ì¸ í•œêµ­ì˜ ë¶„ìœ„ê¸°ë¥¼ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ëœ»í•˜ê³  í¸ì•ˆí•œ ì¡°ëª…ê³¼ í˜„ëŒ€ì ì¸ ë””ìì¸ì´ ì¡°í™”ë¥¼ ì´ë£¨ì–´ ê°€ì¡± ë‹¨ìœ„ ì†ë‹˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—°ì¸ë“¤ì˜ ë°ì´íŠ¸ ì¥ì†Œë¡œë„ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The human asks what the most popular menu items are at the restaurant. The AI responds that the most popular items are bulgogi, seafood pancake, bibimbap, and potato soup, noting that bulgogi is particularly favored by foreign guests for its sweet and salty flavor. The human then inquires about vegetarian options, and the AI confirms that they offer vegetable bibimbap, grilled tofu, vegetable pancakes, and a set of seasoned vegetables made with fresh seasonal produce for a healthy and delicious meal.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})['history'][0].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
